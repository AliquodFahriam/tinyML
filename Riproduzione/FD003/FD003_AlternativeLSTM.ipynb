{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 10:04:05.627863: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 10:04:05.627900: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 10:04:05.627918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 10:04:05.634846: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../CMAPSS/train_FD003.txt\", sep= \"\\s+\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_data_with_targets(input_data, target_data = None, window_length = 1, shift = 1):\n",
    "    \"\"\"Depending on values of window_length and shift, this function generates batchs of data and targets \n",
    "    from input_data and target_data.\n",
    "    \n",
    "    Number of batches = np.floor((len(input_data) - window_length)/shift) + 1\n",
    "    \n",
    "    **We don't check input dimensions uisng exception handling. So readers should be careful while using these\n",
    "    functions. If input data are not of desired dimension, either error occurs or something undesirable is \n",
    "    produced as output.**\n",
    "    \n",
    "    Arguments:\n",
    "        input_data: input data to function (Must be 2 dimensional)\n",
    "        target_data: input rul values (Must be 1D array)s\n",
    "        window_length: window length of data\n",
    "        shift: Distance by which the window moves for next batch. This is closely related to overlap\n",
    "               between data. For example, if window length is 30 and shift is 1, there is an overlap of \n",
    "               29 data points between two consecutive batches.\n",
    "        \n",
    "    \"\"\"\n",
    "    num_batches = int(np.floor((len(input_data) - window_length)/shift)) + 1\n",
    "    num_features = input_data.shape[1]\n",
    "    output_data = np.repeat(np.nan, repeats = num_batches * window_length * num_features).reshape(num_batches, window_length,\n",
    "                                                                                                  num_features)\n",
    "    if target_data is None:\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "        return output_data\n",
    "    else:\n",
    "        output_targets = np.repeat(np.nan, repeats = num_batches)\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "            output_targets[batch] = target_data[(shift*batch + (window_length-1))]\n",
    "        return output_data, output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(test_data_for_an_engine, window_length, shift, num_test_windows = 1):\n",
    "    \"\"\" This function takes test data for an engine as first input. The next two inputs\n",
    "    window_length and shift are same as other functins. \n",
    "    \n",
    "    Finally it takes num_test_windows as the last input. num_test_windows sets how many examplles we\n",
    "    want from test data (from last). By default it extracts only the last example.\n",
    "    \n",
    "    The function return last examples and number of last examples (a scaler) as output. \n",
    "    We need the second output later. If we are extracting more than 1 last examples, we have to \n",
    "    average their prediction results. The second scaler halps us do just that.\n",
    "    \"\"\"\n",
    "    max_num_test_batches = int(np.floor((len(test_data_for_an_engine) - window_length)/shift)) + 1\n",
    "    if max_num_test_batches < num_test_windows:\n",
    "        required_len = (max_num_test_batches -1)* shift + window_length\n",
    "        batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                          target_data = None,\n",
    "                                                                          window_length = window_length, shift = shift)\n",
    "        return batched_test_data_for_an_engine, max_num_test_batches\n",
    "    else:\n",
    "        required_len = (num_test_windows - 1) * shift + window_length\n",
    "        batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                          target_data = None,\n",
    "                                                                          window_length = window_length, shift = shift)\n",
    "        return batched_test_data_for_an_engine, num_test_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_targets(data_length, early_rul = None):\n",
    "    if early_rul == None:\n",
    "        return np.arange(data_length-1, -1, -1)\n",
    "    else:\n",
    "        early_rul_duration = data_length - early_rul\n",
    "        if early_rul_duration <= 0:\n",
    "            return np.arange(data_length-1, -1, -1)\n",
    "        else:\n",
    "            return np.append(early_rul*np.ones(shape = (early_rul_duration,)), np.arange(early_rul-1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed trianing data shape:  (21820, 30, 14)\n",
      "Processed training ruls shape:  (21820,)\n",
      "Processed test data shape:  (500, 30, 14)\n",
      "True RUL shape:  (100,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"../../CMAPSS/test_FD003.txt\", sep = \"\\s+\", header = None)\n",
    "true_rul = pd.read_csv(\"../../CMAPSS/RUL_FD003.txt\", sep = '\\s+', header = None)\n",
    "\n",
    "window_length = 30 #corretta\n",
    "shift = 1 #corretta\n",
    "early_rul = 125        #corretta    \n",
    "processed_train_data = []\n",
    "processed_train_targets = []\n",
    "\n",
    "# How many test windows to take for each engine. If set to 1 (this is the default), only last window of test data for \n",
    "# each engine is taken. If set to a different number, that many windows from last are taken. \n",
    "# Final output is the average output of all windows.\n",
    "num_test_windows = 5     \n",
    "processed_test_data = []\n",
    "num_test_windows_list = []\n",
    "\n",
    "columns_to_be_dropped = [0,1,2,3,4,5,9,10,14,20,22,23]\n",
    "\n",
    "train_data_first_column = train_data[0]\n",
    "test_data_first_column = test_data[0]\n",
    "\n",
    "# Scale data for all engines\n",
    "scaler = MinMaxScaler(feature_range = (-1,1))\n",
    "train_data = scaler.fit_transform(train_data.drop(columns = columns_to_be_dropped))\n",
    "test_data = scaler.transform(test_data.drop(columns = columns_to_be_dropped))\n",
    "\n",
    "train_data = pd.DataFrame(data = np.c_[train_data_first_column, train_data])\n",
    "test_data = pd.DataFrame(data = np.c_[test_data_first_column, test_data])\n",
    "\n",
    "num_train_machines = len(train_data[0].unique())\n",
    "num_test_machines = len(test_data[0].unique())\n",
    "\n",
    "# Process training and test data sepeartely as number of engines in training and test set may be different.\n",
    "# As we are doing scaling for full dataset, we are not bothered by different number of engines in training and test set.\n",
    "\n",
    "# Process trianing data\n",
    "for i in np.arange(1, num_train_machines + 1):\n",
    "    temp_train_data = train_data[train_data[0] == i].drop(columns = [0]).values\n",
    "    \n",
    "    # Verify if data of given window length can be extracted from training data\n",
    "    if (len(temp_train_data) < window_length):\n",
    "        print(\"Train engine {} doesn't have enough data for window_length of {}\".format(i, window_length))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "        \n",
    "    temp_train_targets = process_targets(data_length = temp_train_data.shape[0], early_rul = early_rul)\n",
    "    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(temp_train_data, temp_train_targets, \n",
    "                                                                                window_length = window_length, shift = shift)\n",
    "    \n",
    "    processed_train_data.append(data_for_a_machine)\n",
    "    processed_train_targets.append(targets_for_a_machine)\n",
    "\n",
    "processed_train_data = np.concatenate(processed_train_data)\n",
    "processed_train_targets = np.concatenate(processed_train_targets)\n",
    "\n",
    "# Process test data\n",
    "for i in np.arange(1, num_test_machines + 1):\n",
    "    temp_test_data = test_data[test_data[0] == i].drop(columns = [0]).values\n",
    "    \n",
    "    # Verify if data of given window length can be extracted from test data\n",
    "    if (len(temp_test_data) < window_length):\n",
    "        print(\"Test engine {} doesn't have enough data for window_length of {}\".format(i, window_length))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_data_for_an_engine, num_windows = process_test_data(temp_test_data, window_length = window_length, shift = shift,\n",
    "                                                             num_test_windows = num_test_windows)\n",
    "    \n",
    "    processed_test_data.append(test_data_for_an_engine)\n",
    "    num_test_windows_list.append(num_windows)\n",
    "\n",
    "processed_test_data = np.concatenate(processed_test_data)\n",
    "true_rul = true_rul[0].values\n",
    "\n",
    "# Shuffle training data\n",
    "index = np.random.permutation(len(processed_train_targets))\n",
    "processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]\n",
    "\n",
    "print(\"Processed trianing data shape: \", processed_train_data.shape)\n",
    "print(\"Processed training ruls shape: \", processed_train_targets.shape)\n",
    "print(\"Processed test data shape: \", processed_test_data.shape)\n",
    "print(\"True RUL shape: \", true_rul.shape)\n",
    "\n",
    "print(type(processed_train_data))\n",
    "print(type(processed_test_data))\n",
    "print(type(processed_train_targets))\n",
    "print(type(true_rul))\n",
    "\n",
    "np.save(\"processed_test_data_np\",processed_test_data)\n",
    "np.save(\"processed_test_RUL_np\", true_rul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train data shape:  (17456, 30, 14)\n",
      "Processed validation data shape:  (4364, 30, 14)\n",
      "Processed train targets shape:  (17456,)\n",
      "Processed validation targets shape:  (4364,)\n"
     ]
    }
   ],
   "source": [
    "processed_train_data, processed_val_data, processed_train_targets, processed_val_targets = train_test_split(processed_train_data,\n",
    "                                                                                                            processed_train_targets,\n",
    "                                                                                                            test_size = 0.2,\n",
    "                                                                                                            random_state = 83)\n",
    "print(\"Processed train data shape: \", processed_train_data.shape)\n",
    "print(\"Processed validation data shape: \", processed_val_data.shape)\n",
    "print(\"Processed train targets shape: \", processed_train_targets.shape)\n",
    "print(\"Processed validation targets shape: \", processed_val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(test_data_for_an_engine, window_length, shift, num_test_windows = 1):\n",
    "    \"\"\" This function takes test data for an engine as first input. The next two inputs\n",
    "    window_length and shift are same as other functins. \n",
    "    \n",
    "    Finally it takes num_test_windows as the last input. num_test_windows sets how many examplles we\n",
    "    want from test data (from last). By default it extracts only the last example.\n",
    "    \n",
    "    The function return last examples and number of last examples (a scaler) as output. \n",
    "    We need the second output later. If we are extracting more than 1 last examples, we have to \n",
    "    average their prediction results. The second scaler halps us do just that.\n",
    "    \"\"\"\n",
    "    max_num_test_batches = int(np.floor((len(test_data_for_an_engine) - window_length)/shift)) + 1\n",
    "    if max_num_test_batches < num_test_windows:\n",
    "        required_len = (max_num_test_batches -1)* shift + window_length\n",
    "        batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                          target_data = None,\n",
    "                                                                          window_length = window_length, shift = shift)\n",
    "        return batched_test_data_for_an_engine, max_num_test_batches\n",
    "    else:\n",
    "        required_len = (num_test_windows - 1) * shift + window_length\n",
    "        batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                          target_data = None,\n",
    "                                                                          window_length = window_length, shift = shift)\n",
    "        return batched_test_data_for_an_engine, num_test_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_targets(data_length, early_rul = None):\n",
    "    if early_rul == None:\n",
    "        return np.arange(data_length-1, -1, -1)\n",
    "    else:\n",
    "        early_rul_duration = data_length - early_rul\n",
    "        if early_rul_duration <= 0:\n",
    "            return np.arange(data_length-1, -1, -1)\n",
    "        else:\n",
    "            return np.append(early_rul*np.ones(shape = (early_rul_duration,)), np.arange(early_rul-1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    alpha = 0.2\n",
    "    difference = y_pred - y_true\n",
    "    squared_difference = tf.square(y_pred - y_true)\n",
    "    \n",
    "    # Calcola la loss per ciascun elemento\n",
    "    loss = tf.where(difference < 0, 2 * alpha * squared_difference, 2 * (alpha + (1 - 2 * alpha)) * squared_difference)\n",
    "    \n",
    "    # Calcola la media delle loss\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a custom model\n",
    "class CreateModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm_layer_1 = layers.LSTM(128, input_shape = input_shape, return_sequences = True, activation = \"tanh\")\n",
    "        self.lstm_layer_2 = layers.LSTM(64, activation = \"tanh\", return_sequences= True)\n",
    "        self.lstm_layer_3 = layers.LSTM(32, activation = \"tanh\")\n",
    "        self.dense_1 = layers.Dense(64, activation = \"relu\")\n",
    "        self.dense_2 = layers.Dense(96, activation = \"relu\")\n",
    "        self.dense_3 = layers.Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, input_data, mask = None, **kwargs):\n",
    "        x = self.lstm_layer_1(input_data, mask = mask)\n",
    "        x = self.lstm_layer_2(x)\n",
    "        x = self.lstm_layer_3(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return self.dense_3(x)\n",
    "    \n",
    "def create_compiled_model():\n",
    "    model = Sequential([\n",
    "        layers.LSTM(128, input_shape = (window_length, 14), return_sequences=True, activation = \"tanh\"),\n",
    "        layers.LSTM(64, activation = \"tanh\", return_sequences = True),\n",
    "        layers.LSTM(32, activation = \"tanh\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(96, activation = \"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(loss = custom_loss, optimizer = tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 10:04:08.913170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:08.920894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:08.921045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:08.926898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:08.926960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:08.927074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:09.303391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:09.303447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:09.303456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-15 10:04:09.303493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-15 10:04:09.303512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6562 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:07:00.0, compute capability: 6.1\n",
      "2024-01-15 10:04:23.044070: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "input_shape = (window_length, processed_train_data.shape[2])\n",
    "model = create_compiled_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    elif epoch >= 10 and epoch < 20 :\n",
    "        return 0.001\n",
    "    elif epoch >= 20 and epoch < 30: \n",
    "        return 0.0001\n",
    "    elif epoch >= 30: \n",
    "        return 0.00001\n",
    "    else: \n",
    "        return 0.01; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 10:04:34.087795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-01-15 10:04:35.211515: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe6347feb80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-15 10:04:35.211546: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-15 10:04:35.217979: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-15 10:04:35.300888: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 13s 31ms/step - loss: 2676.9907 - val_loss: 1500.6014 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 1428.0488 - val_loss: 1438.5616 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 1s 17ms/step - loss: 1423.8461 - val_loss: 1436.0424 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 1421.4797 - val_loss: 1437.2697 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 1421.5746 - val_loss: 1436.7173 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 1422.2119 - val_loss: 1410.2241 - lr: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 1s 17ms/step - loss: 612.3006 - val_loss: 197.4698 - lr: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 1s 17ms/step - loss: 179.1205 - val_loss: 169.2460 - lr: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 160.5475 - val_loss: 169.7932 - lr: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 1s 20ms/step - loss: 139.3383 - val_loss: 133.9411 - lr: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 1s 20ms/step - loss: 129.5286 - val_loss: 118.1175 - lr: 0.0010\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 121.7687 - val_loss: 115.8034 - lr: 0.0010\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 2s 22ms/step - loss: 117.7350 - val_loss: 109.3728 - lr: 0.0010\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 1s 20ms/step - loss: 112.7858 - val_loss: 124.3929 - lr: 0.0010\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 105.8407 - val_loss: 102.7440 - lr: 0.0010\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 106.7654 - val_loss: 98.3333 - lr: 0.0010\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 100.5084 - val_loss: 98.1888 - lr: 0.0010\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 98.7962 - val_loss: 97.1869 - lr: 0.0010\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 96.8267 - val_loss: 90.4307 - lr: 0.0010\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 95.7620 - val_loss: 96.9003 - lr: 0.0010\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 86.4497 - val_loss: 86.2313 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 83.8265 - val_loss: 86.4388 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 84.0077 - val_loss: 86.6082 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 82.9356 - val_loss: 85.0823 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 82.2774 - val_loss: 84.0321 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 1s 21ms/step - loss: 81.5527 - val_loss: 82.9607 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 81.2314 - val_loss: 84.3590 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 1s 20ms/step - loss: 81.7605 - val_loss: 84.5098 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 80.5937 - val_loss: 81.8081 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 80.4714 - val_loss: 82.9706 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 1s 20ms/step - loss: 78.9965 - val_loss: 81.1522 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 1s 21ms/step - loss: 78.5694 - val_loss: 81.0212 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 78.4856 - val_loss: 81.0049 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 78.4134 - val_loss: 81.0359 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 1s 20ms/step - loss: 78.3521 - val_loss: 80.9282 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 1s 20ms/step - loss: 78.3838 - val_loss: 80.9473 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 78.2954 - val_loss: 80.7704 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 78.2202 - val_loss: 80.8031 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 78.1076 - val_loss: 80.6707 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 78.1609 - val_loss: 80.6185 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 78.0518 - val_loss: 80.6891 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 77.9949 - val_loss: 80.4843 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 77.9509 - val_loss: 80.5149 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 77.9041 - val_loss: 80.4375 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 1s 17ms/step - loss: 77.8526 - val_loss: 80.4935 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 77.7750 - val_loss: 80.3225 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 77.7373 - val_loss: 80.4524 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 1s 17ms/step - loss: 77.7189 - val_loss: 80.2151 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 77.6070 - val_loss: 80.0865 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 77.6311 - val_loss: 80.2547 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50 \n",
    "BATCH_SIZE = 256\n",
    "\n",
    "history = model.fit(processed_train_data, processed_train_targets, epochs = EPOCHS, \n",
    "                    validation_data = (processed_val_data, processed_val_targets), \n",
    "                    callbacks = callback, \n",
    "                    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 7ms/step\n",
      "RMSE:  15.614357530677063\n"
     ]
    }
   ],
   "source": [
    "rul_pred = model.predict(processed_test_data).reshape(-1)\n",
    "preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
    "mean_pred_for_each_engine = [np.average(ruls_for_each_engine, weights = np.repeat(1/num_windows, num_windows)) \n",
    "                             for ruls_for_each_engine, num_windows in zip(preds_for_each_engine, num_test_windows_list)]\n",
    "RMSE = np.sqrt(mean_squared_error(true_rul, mean_pred_for_each_engine))\n",
    "print(\"RMSE: \", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Taking only last examples):  14.651625527797256\n"
     ]
    }
   ],
   "source": [
    "indices_of_last_examples = np.cumsum(num_test_windows_list) - 1\n",
    "preds_for_last_example = np.concatenate(preds_for_each_engine)[indices_of_last_examples]\n",
    "\n",
    "RMSE_new = np.sqrt(mean_squared_error(true_rul, preds_for_last_example))\n",
    "print(\"RMSE (Taking only last examples): \", RMSE_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_s_score(rul_true, rul_pred):\n",
    "    \"\"\"\n",
    "    Both rul_true and rul_pred should be 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    diff = rul_pred - rul_true\n",
    "    return np.sum(np.where(diff < 0, np.exp(-diff/13)-1, np.exp(diff/10)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-score (LARGE MODEL MSE):  267.5425101752731\n"
     ]
    }
   ],
   "source": [
    "s_score = compute_s_score(true_rul, preds_for_last_example)\n",
    "print(\"S-score (LARGE MODEL MSE): \", s_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "motor_indices = np.arange(100)\n",
    "bar_width = 0.60\n",
    "plt.bar(motor_indices, true_rul, bar_width, label=\"True RUL\", color=\"red\")\n",
    "plt.bar(motor_indices + bar_width , preds_for_last_example, bar_width, label=\"Pred RUL\", color=\"blue\")\n",
    "# Aggiungere legenda e mostrare il grafico\n",
    "plt.legend()\n",
    "plt.xlabel(\"Motori\")\n",
    "plt.ylabel(\"RUL Values\")\n",
    "plt.title(\"Confronto diretto tra True e Predicted RUL per motore\")\n",
    "plt.savefig(\"alternative_LSTM.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
