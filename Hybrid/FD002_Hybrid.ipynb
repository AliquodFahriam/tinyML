{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 12:34:22.525722: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-16 12:34:22.525766: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-16 12:34:22.525807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-16 12:34:22.533760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.26.1\n",
      "('Scikit-learn version: ', '1.3.1')\n",
      "Pandas version:  2.1.1\n",
      "Scikit-learn version:  1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy version: \", np.__version__)\n",
    "print((\"Scikit-learn version: \", sklearn.__version__))\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Scikit-learn version: \", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../CMAPSS/train_FD002.txt\", sep= \"\\s+\", header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_targets(data_length, early_rul = None):\n",
    "    \"\"\" \n",
    "    Takes datalength and earlyrul as input and \n",
    "    creates target rul.\n",
    "    \"\"\"\n",
    "    if early_rul == None:\n",
    "        return np.arange(data_length-1, -1, -1)\n",
    "    else:\n",
    "        early_rul_duration = data_length - early_rul\n",
    "        if early_rul_duration <= 0:\n",
    "            return np.arange(data_length-1, -1, -1)\n",
    "        else:\n",
    "            return np.append(early_rul*np.ones(shape = (early_rul_duration,)), np.arange(early_rul-1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_data_with_targets(input_data, target_data = None, window_length = 1, shift = 1):\n",
    "    \"\"\"Depending on values of window_length and shift, this function generates batchs of data and targets \n",
    "    from input_data and target_data.\n",
    "    \n",
    "    Number of batches = np.floor((len(input_data) - window_length)/shift) + 1\n",
    "    \n",
    "    **We don't check input dimensions uisng exception handling. So readers should be careful while using these\n",
    "    functions. If input data are not of desired dimension, either error occurs or something undesirable is \n",
    "    produced as output.**\n",
    "    \n",
    "    Arguments:\n",
    "        input_data: input data to function (Must be 2 dimensional)\n",
    "        target_data: input rul values (Must be 1D array)s\n",
    "        window_length: window length of data\n",
    "        shift: Distance by which the window moves for next batch. This is closely related to overlap\n",
    "               between data. For example, if window length is 30 and shift is 1, there is an overlap of \n",
    "               29 data points between two consecutive batches.\n",
    "        \n",
    "    \"\"\"\n",
    "    num_batches = int(np.floor((len(input_data) - window_length)/shift)) + 1\n",
    "    num_features = input_data.shape[1]\n",
    "    output_data = np.repeat(np.nan, repeats = num_batches * window_length * num_features).reshape(num_batches, window_length, num_features)\n",
    "    \n",
    "    if target_data is None:\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "        return output_data\n",
    "        \n",
    "    else:\n",
    "        output_targets = np.repeat(np.nan, repeats = num_batches)\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "            output_targets[batch] = target_data[(shift*batch + (window_length-1))]\n",
    "        return output_data, output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(test_data_for_an_engine, window_length, shift, num_test_windows = 1):\n",
    "    \"\"\" This function takes test data for an engine as first input. The next two inputs\n",
    "    window_length and shift are same as other functins. \n",
    "    \n",
    "    Finally it takes num_test_windows as the last input. num_test_windows sets how many examplles we\n",
    "    want from test data (from last). By default it extracts only the last example.\n",
    "    \n",
    "    The function returns last examples, number of last examples (a scaler), a mask, and test_decoder sequence as output. \n",
    "    We need the second output later. If we are extracting more than 1 last examples, we have to \n",
    "    average their prediction results. The second scaler halps us do just that.\n",
    "    \"\"\"\n",
    "    if len(test_data_for_an_engine) < window_length:\n",
    "        mask_length = window_length - len(test_data_for_an_engine)\n",
    "        batched_test_data_for_an_engine = np.concatenate((test_data_for_an_engine,\n",
    "                                                          np.zeros((mask_length, test_data_for_an_engine.shape[1]))))\n",
    "        mask = np.concatenate((np.repeat([True], len(test_data_for_an_engine)), np.repeat([False], mask_length)))\n",
    "        num_test_windows = 1    # This has to be 1 as number of data points is less than window length\n",
    "        return batched_test_data_for_an_engine[np.newaxis, :], num_test_windows, mask[np.newaxis, :]                                                \n",
    "    \n",
    "    else:\n",
    "        max_num_test_batches = int(np.floor((len(test_data_for_an_engine) - window_length)/shift)) + 1\n",
    "        \n",
    "        if max_num_test_batches < num_test_windows:\n",
    "            required_len = (max_num_test_batches -1)* shift + window_length\n",
    "            batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                              target_data = None,\n",
    "                                                                              window_length = window_length, shift = shift)\n",
    "            m, n = batched_test_data_for_an_engine.shape[0], batched_test_data_for_an_engine.shape[1]\n",
    "            mask = np.repeat([True], m * n).reshape(m, n)\n",
    "            return batched_test_data_for_an_engine, max_num_test_batches, mask\n",
    "        \n",
    "        else:\n",
    "            required_len = (num_test_windows - 1) * shift + window_length\n",
    "            batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                              target_data = None,\n",
    "                                                                              window_length = window_length, shift = shift)\n",
    "            m, n = batched_test_data_for_an_engine.shape[0], batched_test_data_for_an_engine.shape[1]\n",
    "            mask = np.repeat([True], m * n).reshape(m, n)\n",
    "            return batched_test_data_for_an_engine, num_test_windows, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed trianing data shape:  (46219, 30, 21)\n",
      "Processed training ruls shape:  (46219,)\n",
      "Processed test data shape:  (1267, 30, 21)\n",
      "Test mask shape:  (1267, 30)\n",
      "True RUL shape:  (259,)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"../CMAPSS/test_FD002.txt\", sep = \"\\s+\", header = None)\n",
    "true_rul = pd.read_csv(\"../CMAPSS/RUL_FD002.txt\", sep = '\\s+', header = None)\n",
    "\n",
    "window_length = 30\n",
    "shift = 1\n",
    "early_rul = 150         \n",
    "processed_train_data = []\n",
    "processed_train_targets = []\n",
    "\n",
    "# How many test windows to take for each engine. If set to 1 (this is the default), only last window of test data for \n",
    "# each engine is taken. If set to a different number, that many windows from last are taken. \n",
    "# Final output is the average output of all windows.\n",
    "num_test_windows = 5     \n",
    "processed_test_data = []\n",
    "num_test_windows_list = []\n",
    "test_mask = []\n",
    "\n",
    "columns_to_be_dropped = [0,1,2,3,4]\n",
    "\n",
    "train_data_first_column = train_data[0]\n",
    "test_data_first_column = test_data[0]\n",
    "\n",
    "# Scale data for all engines\n",
    "# scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data.drop(columns = columns_to_be_dropped))\n",
    "test_data = scaler.transform(test_data.drop(columns = columns_to_be_dropped))\n",
    "\n",
    "train_data = pd.DataFrame(data = np.c_[train_data_first_column, train_data])\n",
    "test_data = pd.DataFrame(data = np.c_[test_data_first_column, test_data])\n",
    "\n",
    "num_train_machines = len(train_data[0].unique())\n",
    "num_test_machines = len(test_data[0].unique())\n",
    "\n",
    "# Process training and test data sepeartely as number of engines in training and test set may be different.\n",
    "# As we are doing scaling for full dataset, we are not bothered by different number of engines in training and test set.\n",
    "\n",
    "# Process trianing data\n",
    "for i in np.arange(1, num_train_machines + 1):\n",
    "    temp_train_data = train_data[train_data[0] == i].drop(columns = [0]).values\n",
    "    \n",
    "    # Verify if data of given window length can be extracted from training data\n",
    "    if (len(temp_train_data) < window_length):\n",
    "        print(\"Train engine {} doesn't have enough data for window_length of {}\".format(i, window_length))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "        \n",
    "    temp_train_targets = process_targets(data_length = temp_train_data.shape[0], early_rul = early_rul)\n",
    "    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(temp_train_data, temp_train_targets, \n",
    "                                                                                window_length = window_length, shift = shift)\n",
    "    \n",
    "    processed_train_data.append(data_for_a_machine)\n",
    "    processed_train_targets.append(targets_for_a_machine)\n",
    "\n",
    "processed_train_data = np.concatenate(processed_train_data)\n",
    "processed_train_targets = np.concatenate(processed_train_targets)\n",
    "\n",
    "# Process test data\n",
    "for i in np.arange(1, num_test_machines + 1):\n",
    "    temp_test_data = test_data[test_data[0] == i].drop(columns = [0]).values\n",
    "\n",
    "\n",
    "    # Prepare test data\n",
    "    test_data_for_an_engine, num_windows, test_mask_for_an_engine = process_test_data(temp_test_data,\n",
    "                                                                                      window_length = window_length,\n",
    "                                                                                      shift = shift,\n",
    "                                                                                      num_test_windows = num_test_windows)\n",
    "    \n",
    "    processed_test_data.append(test_data_for_an_engine)\n",
    "    num_test_windows_list.append(num_windows)\n",
    "    test_mask.append(test_mask_for_an_engine)\n",
    "\n",
    "processed_test_data = np.concatenate(processed_test_data)\n",
    "true_rul = true_rul[0].values\n",
    "test_mask = np.concatenate(test_mask)\n",
    "\n",
    "# Shuffle training data\n",
    "index = np.random.permutation(len(processed_train_targets))\n",
    "processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]\n",
    "\n",
    "print(\"Processed trianing data shape: \", processed_train_data.shape)\n",
    "print(\"Processed training ruls shape: \", processed_train_targets.shape)\n",
    "print(\"Processed test data shape: \", processed_test_data.shape)\n",
    "print(\"Test mask shape: \", test_mask.shape)\n",
    "print(\"True RUL shape: \", true_rul.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "processed_train_targets = target_scaler.fit_transform(processed_train_targets.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train data shape:  (36975, 30, 21)\n",
      "Processed validation data shape:  (9244, 30, 21)\n",
      "Processed train targets shape:  (36975,)\n",
      "Processed validation targets shape:  (9244,)\n"
     ]
    }
   ],
   "source": [
    "processed_train_data, processed_val_data, processed_train_targets, processed_val_targets = train_test_split(processed_train_data,\n",
    "                                                                                                            processed_train_targets,\n",
    "                                                                                                            test_size = 0.2,\n",
    "                                                                                                            random_state = 38)\n",
    "print(\"Processed train data shape: \", processed_train_data.shape)\n",
    "print(\"Processed validation data shape: \", processed_val_data.shape)\n",
    "print(\"Processed train targets shape: \", processed_train_targets.shape)\n",
    "print(\"Processed validation targets shape: \", processed_val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 12:34:25.553551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.558955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.559171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.560598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.560804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.560959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.637994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.638286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.638473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-16 12:34:25.638709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4331 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((processed_train_data, processed_train_targets)).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((processed_val_data, processed_val_targets)).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((processed_test_data, test_mask)).batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    alpha = 0.4\n",
    "    difference = y_pred - y_true\n",
    "    squared_difference = tf.square(y_pred - y_true)\n",
    "\n",
    "    # Calcola la loss per ciascun elemento\n",
    "    loss = tf.where(difference < 0, 2 * alpha * squared_difference, 2 * (alpha + (1 - 2 * alpha)) * squared_difference)\n",
    "\n",
    "    # Calcola la media delle loss\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " cnn_input (InputLayer)      [(None, 30, 21)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 22, 10)               1900      ['cnn_input[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPoolin  (None, 11, 10)               0         ['conv1d_12[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " lstm_input (InputLayer)     [(None, 30, 21)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 9, 10)                310       ['max_pooling1d_8[0][0]']     \n",
      "                                                                                                  \n",
      " lstm_12 (LSTM)              (None, 30, 32)               6912      ['lstm_input[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPoolin  (None, 4, 10)                0         ['conv1d_13[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)              (None, 30, 32)               8320      ['lstm_12[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 2, 128)               3968      ['max_pooling1d_9[0][0]']     \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)              (None, 64)                   24832     ['lstm_13[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)         (None, 256)                  0         ['conv1d_14[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 320)                  0         ['lstm_14[0][0]',             \n",
      " )                                                                   'flatten_5[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 100)                  32100     ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   3232      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    33        ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 81607 (318.78 KB)\n",
      "Trainable params: 81607 (318.78 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate,Conv2D\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "def build_lstm_cnn_model(timesteps, features, sequence_length):\n",
    "    # Definizione dell'input per la parte LSTM\n",
    "    lstm_input = Input(shape=(timesteps, features), name='lstm_input')\n",
    "\n",
    "    # Aggiunta di tre layer LSTM\n",
    "    lstm_layer1 = LSTM(units=32, activation='relu', return_sequences=True)(lstm_input)\n",
    "    lstm_layer2 = LSTM(units=32, activation='relu', return_sequences=True)(lstm_layer1)\n",
    "    lstm_layer3 = LSTM(units=64, activation='relu')(lstm_layer2)\n",
    "    flatten = Flatten()(lstm_layer3)\n",
    "    # Creazione del modello LSTM\n",
    "    lstm_model = Model(inputs=lstm_input, outputs=lstm_layer3)\n",
    "\n",
    "    # Definizione dell'input per la parte CNN\n",
    "    cnn_input = Input(shape=(sequence_length, features), name='cnn_input')\n",
    "\n",
    "    # Aggiunta di tre layer CNN\n",
    "    conv1d_layer1 = Conv1D(filters=10, kernel_size=9, activation='relu')(cnn_input)\n",
    "    maxpooling_layer1 = MaxPooling1D(pool_size=2)(conv1d_layer1)\n",
    "\n",
    "    conv1d_layer2 = Conv1D(filters=10, kernel_size=3, activation='relu')(maxpooling_layer1)\n",
    "    maxpooling_layer2 = MaxPooling1D(pool_size=2)(conv1d_layer2)\n",
    "\n",
    "    conv1d_layer3 = Conv1D(filters=128, kernel_size=3, activation='relu')(maxpooling_layer2)\n",
    "    flatten_cnn = Flatten()(conv1d_layer3)\n",
    "\n",
    "    # Concatenazione delle uscite di LSTM e CNN\n",
    "    concatenated = Concatenate()([lstm_model.output, flatten_cnn])\n",
    "\n",
    "    # Aggiunta di uno o più strati completamente connessi per la regressione\n",
    "    dense_layer1 = Dense(100, activation='tanh')(concatenated)\n",
    "    dense_layer2 = Dense(32, activation='tanh')(dense_layer1)\n",
    "    output_layer = Dense(1, activation='relu')(dense_layer2)\n",
    "\n",
    "    # Creazione del modello finale\n",
    "    model = Model(inputs=[lstm_input, cnn_input], outputs=output_layer)\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Esempio di utilizzo della funzione per creare e compilare il modello\n",
    "timesteps = 30  # Imposta il valore appropriato\n",
    "features = 21   # Imposta il valore appropriato\n",
    "sequence_length = 30  # Imposta il valore appropriato\n",
    "\n",
    "model = build_lstm_cnn_model(timesteps, features, sequence_length)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 12:37:32.312900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-01-16 12:37:32.438696: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-16 12:37:33.278870: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6e0caa7d20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-16 12:37:33.278955: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2060 with Max-Q Design, Compute Capability 7.5\n",
      "2024-01-16 12:37:33.288569: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-16 12:37:33.385882: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 15s 129ms/step - loss: 0.1042 - root_mean_squared_error: 0.3227 - val_loss: 0.0754 - val_root_mean_squared_error: 0.2745\n",
      "Epoch 2/20\n",
      "73/73 [==============================] - 8s 117ms/step - loss: 0.0433 - root_mean_squared_error: 0.2081 - val_loss: 0.0351 - val_root_mean_squared_error: 0.1873\n",
      "Epoch 3/20\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 0.0356 - root_mean_squared_error: 0.1887 - val_loss: 0.0337 - val_root_mean_squared_error: 0.1837\n",
      "Epoch 4/20\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 0.0338 - root_mean_squared_error: 0.1838 - val_loss: 0.0369 - val_root_mean_squared_error: 0.1920\n",
      "Epoch 5/20\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 0.0322 - root_mean_squared_error: 0.1794 - val_loss: 0.0309 - val_root_mean_squared_error: 0.1758\n",
      "Epoch 6/20\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 0.0313 - root_mean_squared_error: 0.1770 - val_loss: 0.0320 - val_root_mean_squared_error: 0.1788\n",
      "Epoch 7/20\n",
      "73/73 [==============================] - 9s 127ms/step - loss: 0.0304 - root_mean_squared_error: 0.1744 - val_loss: 0.0308 - val_root_mean_squared_error: 0.1754\n",
      "Epoch 8/20\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 0.0290 - root_mean_squared_error: 0.1703 - val_loss: 0.0279 - val_root_mean_squared_error: 0.1670\n",
      "Epoch 9/20\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 0.0274 - root_mean_squared_error: 0.1655 - val_loss: 0.0398 - val_root_mean_squared_error: 0.1996\n",
      "Epoch 10/20\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 0.0266 - root_mean_squared_error: 0.1631 - val_loss: 0.0253 - val_root_mean_squared_error: 0.1592\n",
      "Epoch 11/20\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 0.0254 - root_mean_squared_error: 0.1593 - val_loss: 0.0247 - val_root_mean_squared_error: 0.1571\n",
      "Epoch 12/20\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 0.0238 - root_mean_squared_error: 0.1543 - val_loss: 0.0246 - val_root_mean_squared_error: 0.1569\n",
      "Epoch 13/20\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 0.0232 - root_mean_squared_error: 0.1525 - val_loss: 0.0265 - val_root_mean_squared_error: 0.1627\n",
      "Epoch 14/20\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 0.0230 - root_mean_squared_error: 0.1518 - val_loss: 0.0228 - val_root_mean_squared_error: 0.1510\n",
      "Epoch 15/20\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 0.0216 - root_mean_squared_error: 0.1471 - val_loss: 0.0226 - val_root_mean_squared_error: 0.1504\n",
      "Epoch 16/20\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 0.0216 - root_mean_squared_error: 0.1470 - val_loss: 0.0225 - val_root_mean_squared_error: 0.1498\n",
      "Epoch 17/20\n",
      "73/73 [==============================] - 9s 120ms/step - loss: 0.0209 - root_mean_squared_error: 0.1447 - val_loss: 0.0211 - val_root_mean_squared_error: 0.1452\n",
      "Epoch 18/20\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 0.0202 - root_mean_squared_error: 0.1422 - val_loss: 0.0208 - val_root_mean_squared_error: 0.1443\n",
      "Epoch 19/20\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 0.0199 - root_mean_squared_error: 0.1412 - val_loss: 0.0222 - val_root_mean_squared_error: 0.1489\n",
      "Epoch 20/20\n",
      "73/73 [==============================] - 9s 124ms/step - loss: 0.0196 - root_mean_squared_error: 0.1402 - val_loss: 0.0201 - val_root_mean_squared_error: 0.1418\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "history = model.fit(\n",
    "    [processed_train_data, processed_train_data],\n",
    "    processed_train_targets,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=([processed_val_data, processed_val_data], processed_val_targets),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_pred_scaled = []\n",
    "for test_data, test_mask in test_dataset:\n",
    "    batch_pred = model([test_data,test_data], mask = [test_mask, test_mask])\n",
    "    rul_pred_scaled.extend(batch_pred.numpy().reshape(-1))\n",
    "\n",
    "rul_pred_scaled = np.array(rul_pred_scaled) # initially rul_pred_scaled was just a list. So converted it to numpy array.\n",
    "rul_pred = target_scaler.inverse_transform(rul_pred_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  27.356878937078562\n",
      "MAE:  19.042670663995636\n"
     ]
    }
   ],
   "source": [
    "preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
    "mean_pred_for_each_engine = [np.average(ruls_for_each_engine, weights = np.repeat(1/num_windows, num_windows)) \n",
    "                             for ruls_for_each_engine, num_windows in zip(preds_for_each_engine, num_test_windows_list)]\n",
    "RMSE = np.sqrt(mean_squared_error(true_rul, mean_pred_for_each_engine))\n",
    "MAE = mean_absolute_error(true_rul, mean_pred_for_each_engine)\n",
    "print(\"RMSE: \", RMSE)\n",
    "print(\"MAE: \", MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Taking only last examples):  27.712899303998984\n",
      "MAE (Taking only last example):  18.931747304886926\n"
     ]
    }
   ],
   "source": [
    "indices_of_last_examples = np.cumsum(num_test_windows_list) - 1\n",
    "preds_for_last_example = np.concatenate(preds_for_each_engine)[indices_of_last_examples]\n",
    "\n",
    "RMSE_new = np.sqrt(mean_squared_error(true_rul, preds_for_last_example))\n",
    "MAE_new = mean_absolute_error(true_rul, preds_for_last_example)\n",
    "print(\"RMSE (Taking only last examples): \", RMSE_new)\n",
    "print(\"MAE (Taking only last example): \", MAE_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_s_score(rul_true, rul_pred):\n",
    "    \"\"\"\n",
    "    Both rul_true and rul_pred should be 1D numpy arrays.\n",
    "    \"\"\"\n",
    "    diff = rul_pred - rul_true\n",
    "    return np.sum(np.where(diff < 0, np.exp(-diff/13)-1, np.exp(diff/10)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-score:  15739.413812190085\n"
     ]
    }
   ],
   "source": [
    "s_score = compute_s_score(true_rul, preds_for_last_example)\n",
    "print(\"S-score: \", s_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
