{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 10:01:43.850373: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-16 10:01:43.850409: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-16 10:01:43.850425: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-16 10:01:43.856638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.26.1\n",
      "('Scikit-learn version: ', '1.3.2')\n",
      "Pandas version:  2.1.1\n",
      "Scikit-learn version:  1.3.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy version: \", np.__version__)\n",
    "print((\"Scikit-learn version: \", sklearn.__version__))\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Scikit-learn version: \", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../CMAPSS/train_FD002.txt\", sep= \"\\s+\", header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_targets(data_length, early_rul = None):\n",
    "    \"\"\" \n",
    "    Takes datalength and earlyrul as input and \n",
    "    creates target rul.\n",
    "    \"\"\"\n",
    "    if early_rul == None:\n",
    "        return np.arange(data_length-1, -1, -1)\n",
    "    else:\n",
    "        early_rul_duration = data_length - early_rul\n",
    "        if early_rul_duration <= 0:\n",
    "            return np.arange(data_length-1, -1, -1)\n",
    "        else:\n",
    "            return np.append(early_rul*np.ones(shape = (early_rul_duration,)), np.arange(early_rul-1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_data_with_targets(input_data, target_data = None, window_length = 1, shift = 1):\n",
    "    \"\"\"Depending on values of window_length and shift, this function generates batchs of data and targets \n",
    "    from input_data and target_data.\n",
    "    \n",
    "    Number of batches = np.floor((len(input_data) - window_length)/shift) + 1\n",
    "    \n",
    "    **We don't check input dimensions uisng exception handling. So readers should be careful while using these\n",
    "    functions. If input data are not of desired dimension, either error occurs or something undesirable is \n",
    "    produced as output.**\n",
    "    \n",
    "    Arguments:\n",
    "        input_data: input data to function (Must be 2 dimensional)\n",
    "        target_data: input rul values (Must be 1D array)s\n",
    "        window_length: window length of data\n",
    "        shift: Distance by which the window moves for next batch. This is closely related to overlap\n",
    "               between data. For example, if window length is 30 and shift is 1, there is an overlap of \n",
    "               29 data points between two consecutive batches.\n",
    "        \n",
    "    \"\"\"\n",
    "    num_batches = int(np.floor((len(input_data) - window_length)/shift)) + 1\n",
    "    num_features = input_data.shape[1]\n",
    "    output_data = np.repeat(np.nan, repeats = num_batches * window_length * num_features).reshape(num_batches, window_length, num_features)\n",
    "    \n",
    "    if target_data is None:\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "        return output_data\n",
    "        \n",
    "    else:\n",
    "        output_targets = np.repeat(np.nan, repeats = num_batches)\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "            output_targets[batch] = target_data[(shift*batch + (window_length-1))]\n",
    "        return output_data, output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data(test_data_for_an_engine, window_length, shift, num_test_windows = 1):\n",
    "    \"\"\" This function takes test data for an engine as first input. The next two inputs\n",
    "    window_length and shift are same as other functins. \n",
    "    \n",
    "    Finally it takes num_test_windows as the last input. num_test_windows sets how many examplles we\n",
    "    want from test data (from last). By default it extracts only the last example.\n",
    "    \n",
    "    The function returns last examples, number of last examples (a scaler), a mask, and test_decoder sequence as output. \n",
    "    We need the second output later. If we are extracting more than 1 last examples, we have to \n",
    "    average their prediction results. The second scaler halps us do just that.\n",
    "    \"\"\"\n",
    "    if len(test_data_for_an_engine) < window_length:\n",
    "        mask_length = window_length - len(test_data_for_an_engine)\n",
    "        batched_test_data_for_an_engine = np.concatenate((test_data_for_an_engine,\n",
    "                                                          np.zeros((mask_length, test_data_for_an_engine.shape[1]))))\n",
    "        mask = np.concatenate((np.repeat([True], len(test_data_for_an_engine)), np.repeat([False], mask_length)))\n",
    "        num_test_windows = 1    # This has to be 1 as number of data points is less than window length\n",
    "        return batched_test_data_for_an_engine[np.newaxis, :], num_test_windows, mask[np.newaxis, :]                                                \n",
    "    \n",
    "    else:\n",
    "        max_num_test_batches = int(np.floor((len(test_data_for_an_engine) - window_length)/shift)) + 1\n",
    "        \n",
    "        if max_num_test_batches < num_test_windows:\n",
    "            required_len = (max_num_test_batches -1)* shift + window_length\n",
    "            batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                              target_data = None,\n",
    "                                                                              window_length = window_length, shift = shift)\n",
    "            m, n = batched_test_data_for_an_engine.shape[0], batched_test_data_for_an_engine.shape[1]\n",
    "            mask = np.repeat([True], m * n).reshape(m, n)\n",
    "            return batched_test_data_for_an_engine, max_num_test_batches, mask\n",
    "        \n",
    "        else:\n",
    "            required_len = (num_test_windows - 1) * shift + window_length\n",
    "            batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                              target_data = None,\n",
    "                                                                              window_length = window_length, shift = shift)\n",
    "            m, n = batched_test_data_for_an_engine.shape[0], batched_test_data_for_an_engine.shape[1]\n",
    "            mask = np.repeat([True], m * n).reshape(m, n)\n",
    "            return batched_test_data_for_an_engine, num_test_windows, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed trianing data shape:  (46219, 30, 21)\n",
      "Processed training ruls shape:  (46219,)\n",
      "Processed test data shape:  (1267, 30, 21)\n",
      "Test mask shape:  (1267, 30)\n",
      "True RUL shape:  (259,)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"../CMAPSS/test_FD002.txt\", sep = \"\\s+\", header = None)\n",
    "true_rul = pd.read_csv(\"../CMAPSS/RUL_FD002.txt\", sep = '\\s+', header = None)\n",
    "\n",
    "window_length = 30\n",
    "shift = 1\n",
    "early_rul = 150         \n",
    "processed_train_data = []\n",
    "processed_train_targets = []\n",
    "\n",
    "# How many test windows to take for each engine. If set to 1 (this is the default), only last window of test data for \n",
    "# each engine is taken. If set to a different number, that many windows from last are taken. \n",
    "# Final output is the average output of all windows.\n",
    "num_test_windows = 5     \n",
    "processed_test_data = []\n",
    "num_test_windows_list = []\n",
    "test_mask = []\n",
    "\n",
    "columns_to_be_dropped = [0,1,2,3,4]\n",
    "\n",
    "train_data_first_column = train_data[0]\n",
    "test_data_first_column = test_data[0]\n",
    "\n",
    "# Scale data for all engines\n",
    "# scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data.drop(columns = columns_to_be_dropped))\n",
    "test_data = scaler.transform(test_data.drop(columns = columns_to_be_dropped))\n",
    "\n",
    "train_data = pd.DataFrame(data = np.c_[train_data_first_column, train_data])\n",
    "test_data = pd.DataFrame(data = np.c_[test_data_first_column, test_data])\n",
    "\n",
    "num_train_machines = len(train_data[0].unique())\n",
    "num_test_machines = len(test_data[0].unique())\n",
    "\n",
    "# Process training and test data sepeartely as number of engines in training and test set may be different.\n",
    "# As we are doing scaling for full dataset, we are not bothered by different number of engines in training and test set.\n",
    "\n",
    "# Process trianing data\n",
    "for i in np.arange(1, num_train_machines + 1):\n",
    "    temp_train_data = train_data[train_data[0] == i].drop(columns = [0]).values\n",
    "    \n",
    "    # Verify if data of given window length can be extracted from training data\n",
    "    if (len(temp_train_data) < window_length):\n",
    "        print(\"Train engine {} doesn't have enough data for window_length of {}\".format(i, window_length))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "        \n",
    "    temp_train_targets = process_targets(data_length = temp_train_data.shape[0], early_rul = early_rul)\n",
    "    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(temp_train_data, temp_train_targets, \n",
    "                                                                                window_length = window_length, shift = shift)\n",
    "    \n",
    "    processed_train_data.append(data_for_a_machine)\n",
    "    processed_train_targets.append(targets_for_a_machine)\n",
    "\n",
    "processed_train_data = np.concatenate(processed_train_data)\n",
    "processed_train_targets = np.concatenate(processed_train_targets)\n",
    "\n",
    "# Process test data\n",
    "for i in np.arange(1, num_test_machines + 1):\n",
    "    temp_test_data = test_data[test_data[0] == i].drop(columns = [0]).values\n",
    "\n",
    "\n",
    "    # Prepare test data\n",
    "    test_data_for_an_engine, num_windows, test_mask_for_an_engine = process_test_data(temp_test_data,\n",
    "                                                                                      window_length = window_length,\n",
    "                                                                                      shift = shift,\n",
    "                                                                                      num_test_windows = num_test_windows)\n",
    "    \n",
    "    processed_test_data.append(test_data_for_an_engine)\n",
    "    num_test_windows_list.append(num_windows)\n",
    "    test_mask.append(test_mask_for_an_engine)\n",
    "\n",
    "processed_test_data = np.concatenate(processed_test_data)\n",
    "true_rul = true_rul[0].values\n",
    "test_mask = np.concatenate(test_mask)\n",
    "\n",
    "# Shuffle training data\n",
    "index = np.random.permutation(len(processed_train_targets))\n",
    "processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]\n",
    "\n",
    "print(\"Processed trianing data shape: \", processed_train_data.shape)\n",
    "print(\"Processed training ruls shape: \", processed_train_targets.shape)\n",
    "print(\"Processed test data shape: \", processed_test_data.shape)\n",
    "print(\"Test mask shape: \", test_mask.shape)\n",
    "print(\"True RUL shape: \", true_rul.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "processed_train_targets = target_scaler.fit_transform(processed_train_targets.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train data shape:  (36975, 30, 21)\n",
      "Processed validation data shape:  (9244, 30, 21)\n",
      "Processed train targets shape:  (36975,)\n",
      "Processed validation targets shape:  (9244,)\n"
     ]
    }
   ],
   "source": [
    "processed_train_data, processed_val_data, processed_train_targets, processed_val_targets = train_test_split(processed_train_data,\n",
    "                                                                                                            processed_train_targets,\n",
    "                                                                                                            test_size = 0.2,\n",
    "                                                                                                            random_state = 38)\n",
    "print(\"Processed train data shape: \", processed_train_data.shape)\n",
    "print(\"Processed validation data shape: \", processed_val_data.shape)\n",
    "print(\"Processed train targets shape: \", processed_train_targets.shape)\n",
    "print(\"Processed validation targets shape: \", processed_val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((processed_train_data, processed_train_targets)).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((processed_val_data, processed_val_targets)).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((processed_test_data, test_mask)).batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    alpha = 0.2\n",
    "    difference = y_pred - y_true\n",
    "    squared_difference = tf.square(y_pred - y_true)\n",
    "\n",
    "    # Calcola la loss per ciascun elemento\n",
    "    loss = tf.where(difference < 0, 2 * alpha * squared_difference, 2 * (alpha + (1 - 2 * alpha)) * squared_difference)\n",
    "\n",
    "    # Calcola la media delle loss\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " cnn_input (InputLayer)      [(None, 30, 21)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 22, 32)               6080      ['cnn_input[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPoolin  (None, 11, 32)               0         ['conv1d_3[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 9, 64)                6208      ['max_pooling1d_3[0][0]']     \n",
      "                                                                                                  \n",
      " lstm_input (InputLayer)     [(None, 30, 21)]             0         []                            \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPoolin  (None, 4, 64)                0         ['conv1d_4[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               (None, 30, 256)              284672    ['lstm_input[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 2, 128)               24704     ['max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)               (None, 30, 128)              197120    ['lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPoolin  (None, 1, 128)               0         ['conv1d_5[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)               (None, 64)                   49408     ['lstm_4[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 128)                  0         ['max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 192)                  0         ['lstm_5[0][0]',              \n",
      " )                                                                   'flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 64)                   12352     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 32)                   2080      ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    33        ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 582657 (2.22 MB)\n",
      "Trainable params: 582657 (2.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "def build_lstm_cnn_model(timesteps, features, sequence_length):\n",
    "    # Definizione dell'input per la parte LSTM\n",
    "    lstm_input = Input(shape=(timesteps, features), name='lstm_input')\n",
    "\n",
    "    # Aggiunta di tre layer LSTM\n",
    "    lstm_layer1 = LSTM(units=256, activation='relu', return_sequences=True)(lstm_input)\n",
    "    lstm_layer2 = LSTM(units=128, activation='relu', return_sequences=True)(lstm_layer1)\n",
    "    lstm_layer3 = LSTM(units=64, activation='relu')(lstm_layer2)\n",
    "\n",
    "    # Creazione del modello LSTM\n",
    "    lstm_model = Model(inputs=lstm_input, outputs=lstm_layer3)\n",
    "\n",
    "    # Definizione dell'input per la parte CNN\n",
    "    cnn_input = Input(shape=(sequence_length, features), name='cnn_input')\n",
    "\n",
    "    # Aggiunta di tre layer CNN\n",
    "    conv1d_layer1 = Conv1D(filters=32, kernel_size=9, activation='relu')(cnn_input)\n",
    "    maxpooling_layer1 = MaxPooling1D(pool_size=2)(conv1d_layer1)\n",
    "\n",
    "    conv1d_layer2 = Conv1D(filters=64, kernel_size=3, activation='relu')(maxpooling_layer1)\n",
    "    maxpooling_layer2 = MaxPooling1D(pool_size=2)(conv1d_layer2)\n",
    "\n",
    "    conv1d_layer3 = Conv1D(filters=128, kernel_size=3, activation='relu')(maxpooling_layer2)\n",
    "    maxpooling_layer3 = MaxPooling1D(pool_size=2)(conv1d_layer3)\n",
    "    flatten_layer3 = Flatten()(maxpooling_layer3)\n",
    "\n",
    "    # Concatenazione delle uscite di LSTM e CNN\n",
    "    concatenated = Concatenate()([lstm_model.output, flatten_layer3])\n",
    "\n",
    "    # Aggiunta di uno o più strati completamente connessi per la regressione\n",
    "    dense_layer1 = Dense(64, activation='relu')(concatenated)\n",
    "    dense_layer2 = Dense(32, activation='relu')(dense_layer1)\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer2)\n",
    "\n",
    "    # Creazione del modello finale\n",
    "    model = Model(inputs=[lstm_input, cnn_input], outputs=output_layer)\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer='adam', loss=custom_loss, metrics=[RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Esempio di utilizzo della funzione per creare e compilare il modello\n",
    "timesteps = 30  # Imposta il valore appropriato\n",
    "features = 21   # Imposta il valore appropriato\n",
    "sequence_length = 30  # Imposta il valore appropriato\n",
    "\n",
    "model = build_lstm_cnn_model(timesteps, features, sequence_length)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 09:56:06.888757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-01-16 09:56:07.394633: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-01-16 09:56:09.342132: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15f8f900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-16 09:56:09.342160: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-01-16 09:56:09.358582: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-16 09:56:09.455409: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23/145 [===>..........................] - ETA: 39s - loss: 0.0952 - root_mean_squared_error: 0.4265"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "history = model.fit(\n",
    "    [processed_train_data, processed_train_data],\n",
    "    processed_train_targets,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=([processed_val_data, processed_val_data], processed_val_targets),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_pred_scaled = []\n",
    "for test_data, test_mask in test_dataset:\n",
    "    batch_pred = model(test_data, mask = test_mask)\n",
    "    rul_pred_scaled.extend(batch_pred.numpy().reshape(-1))\n",
    "\n",
    "rul_pred_scaled = np.array(rul_pred_scaled) # initially rul_pred_scaled was just a list. So converted it to numpy array.\n",
    "rul_pred = target_scaler.inverse_transform(rul_pred_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
